{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Library Setup__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import ibm_db\n",
    "import ibm_db_dbi\n",
    "from django.shortcuts import get_object_or_404\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "# show plots in the notebook\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\clauser\\\\Desktop\\\\CLA\\\\Jeanine\\\\Data Extract\\\\DE Extract Daily Tracking'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current directory\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obtain Extract from AWS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today is 2018_03_18\n"
     ]
    }
   ],
   "source": [
    "# today_\n",
    "today_ = datetime.date.today().strftime(\"%Y_%m_%d\")\n",
    "# today_ = '2018_03_05'\n",
    "print(\"today is \" + today_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name: cla/extracts/ccextract_CLA_origination_2018_03_18_11_00_04.zip, file size: 100.1 MB\n"
     ]
    }
   ],
   "source": [
    "# connect to AWS cloudberry\n",
    "aws_dict = {'AWS_BUCKET_KEY': 'AKIAIP4Z3G3YKK4X3IQA',\n",
    "           'AWS_SECRET_KEY': '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'}\n",
    "\n",
    "client = boto3.client('s3',\n",
    "                   aws_access_key_id=aws_dict['AWS_BUCKET_KEY'], \n",
    "                    aws_secret_access_key=aws_dict['AWS_SECRET_KEY'])\n",
    "\n",
    "resource = boto3.resource('s3', aws_access_key_id=aws_dict['AWS_BUCKET_KEY'], \n",
    "                    aws_secret_access_key=aws_dict['AWS_SECRET_KEY'])\n",
    "\n",
    "my_bucket = resource.Bucket('dataview-systems-prod')\n",
    "\n",
    "\"\"\"Get all files key in folder cla/extracts\"\"\"\n",
    "files = client.list_objects(Bucket = 'dataview-systems-prod', Prefix = 'cla/extracts')\n",
    "keys = [[x['Key'], x['Size']] for x in files['Contents']]\n",
    "keys[:5]\n",
    "\n",
    "\"\"\"Search through keys for date 2018_xx_xx data\"\"\"\n",
    "keys_today_ = [x for x in keys if x[0].find(today_) != -1]\n",
    "for i in keys_today_:\n",
    "    print(\"file name: %s, file size: %.1f MB\"%(i[0], i[1]/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# import extract and de-dup\n",
    "def f_import_extract_and_dedup(import_filename, dedup=False, zipped=True):\n",
    "    \n",
    "    # import extract\n",
    "    key = import_filename \n",
    "    if zipped==True:\n",
    "        obj = client.get_object(Bucket='dataview-systems-prod', Key=key)\n",
    "        data = pd.read_csv(io.BytesIO(obj['Body'].read()), compression = 'zip', sep = ';')\n",
    "    else:\n",
    "        data = pd.read_csv(key, sep=';')\n",
    "    \n",
    "    # indexing\n",
    "    data['extract_idx'] = range(len(data))\n",
    "    \n",
    "    # replace missing with other values to avoid mistakes in de-dup due to numpy ignoring NaN\n",
    "    data.loc[data.SELECTED_LOAN_AMT.isnull(),'SELECTED_LOAN_AMT'] = -1\n",
    "    data.loc[data.CL_1_CF_score.isnull(), 'CL_1_CF_score'] = -1\n",
    "    data.loc[data.DEC_CREDIT_BUREAU_USED.isnull(), 'DEC_CREDIT_BUREAU_USED'] = 'No Credit Bureau Used'\n",
    "    data.loc[data.DEC_CHAMPION_CHALLENGER_PATH.isnull(), 'DEC_CHAMPION_CHALLENGER_PATH'] = 'No Champ Path'\n",
    "    data.loc[data.APP_SOURCE.isnull(),'APP_SOURCE'] = '(null)'\n",
    "    data.loc[data.LEAD_REFERRING_URL.isnull(), 'LEAD_REFERRING_URL'] = '(null)'\n",
    "    \n",
    "    \n",
    "    ########################### add/reformat fields ####################################################\n",
    "    #     data['APP_CELL_PHONE_NUMBER'] = [str(int(x)) if str(x) not in ['NaT','nan','(null)','']\n",
    "    #                                                  else '(null)'\n",
    "    #                                     for x in data.APP_CELL_PHONE_NUMBER.values]\n",
    "    #     data['APP_HOME_PHONE_NUMBER'] = [str(int(x)) if str(x) not in ['NaT','nan','(null)','']\n",
    "    #                                                  else '(null)'\n",
    "    #                                     for x in data.APP_HOME_PHONE_NUMBER.values]\n",
    "    \n",
    "    data['existing_customer_flag'] = 0\n",
    "    data.loc[~data.CTL_EXISTING_FLAG.isnull(),'existing_customer_flag'] = 1\n",
    "    \n",
    "    lct = time.localtime()\n",
    "    today_year = lct.tm_year\n",
    "    data['app_orig_veh_age_clean'] = np.nan\n",
    "    i = 0\n",
    "    for x in data.APP_ORIG_VEH_AGE.values:\n",
    "        try:\n",
    "            if str(x)!='nan':\n",
    "                if int(x)<=today_year:\n",
    "                    data.loc[i, 'app_orig_veh_age_clean'] = int(x)\n",
    "        except ValueError:\n",
    "            data.loc[i, 'app_orig_veh_age_clean'] = np.nan\n",
    "        i = i + 1\n",
    "    \n",
    "    data['LEAD_REFERRING_URL_short'] = [x if x=='(null)'\n",
    "                                           else (re.search(\"(?P<url>https?://[^\\s]+.com)\", x).group(\"url\")\n",
    "                                                 if ((re.search(\"(?P<url>https?://[^\\s]+.com)\", x)!=None))\n",
    "                                                 else (re.search(\"(?P<url>https?://[^\\s]+.net)\", x).group(\"url\")\n",
    "                                                       if ((re.search(\"(?P<url>https?://[^\\s]+.net)\", x)!=None))\n",
    "                                                       else (re.search(\"(?P<url>https?://[^\\s]+.org)\", x).group(\"url\")\n",
    "                                                            if ((re.search(\"(?P<url>https?://[^\\s]+.org)\", x)!=None))\n",
    "                                                            else (re.search(\"(?P<url>https?://[^\\s]+.co)\", x).group(\"url\")\n",
    "                                                                  if ((re.search(\"(?P<url>https?://[^\\s]+.co)\", x)!=None))\n",
    "                                                                  else (re.search(\"(?P<url>https?://[^\\s]+.loans)\", x).group(\"url\")\n",
    "                                                                        if ((re.search(\"(?P<url>https?://[^\\s]+.loans)\", x)!=None))\n",
    "                                                                        else re.search(\"(?P<url>https?://[^\\s]+.cash)\", x).group(\"url\")\n",
    "                                                                             if ((re.search(\"(?P<url>https?://[^\\s]+.cash)\", x)!=None))\n",
    "                                                                             else (re.search(\"(?P<url>https?://[^\\s]+.direct)\", x).group(\"url\")\n",
    "                                                                                   if ((re.search(\"(?P<url>https?://[^\\s]+.direct)\", x)!=None))\n",
    "                                                                                   else (re.search(\"(?P<url>https?://[^\\s]+.store)\", x).group(\"url\")\n",
    "                                                                                         if ((re.search(\"(?P<url>https?://[^\\s]+.store)\", x)!=None))\n",
    "                                                                                         else (re.search(\"(?P<url>https?://[^\\s]+.today)\", x).group(\"url\")\n",
    "                                                                                               if ((re.search(\"(?P<url>https?://[^\\s]+.today)\", x)!=None))\n",
    "                                                                                               else (re.search(\"(?P<url>https?://[^\\s]+.us)\", x).group(\"url\")\n",
    "                                                                                                     if ((re.search(\"(?P<url>https?://[^\\s]+.us)\", x)!=None))\n",
    "                                                                                                     else np.nan)))))))))\n",
    "                                       for x in data.LEAD_REFERRING_URL.values]\n",
    "    \n",
    "    data['APPLICATION_DATE_short'] = [datetime.date(int(str(x)[0:4]),int(str(x)[5:7]), int(str(x)[8:10])) \n",
    "                                       if str(x)!='nan' \n",
    "                                       else np.nan\n",
    "                                       for x in data.APPLICATION_DATE.values]\n",
    "    data['application_month'] = [str(x)[0:7] for x in data.APPLICATION_DATE]\n",
    "    \n",
    "    # app_source_v2\n",
    "    leadgen = ['DOT818','Dot818', 'DOT1818', 'LEADSMKT']\n",
    "    data['APP_LOGIN_ID_cap'] = [str.upper(str(x)).rstrip() if str(x)!='nan' else 'nan' for x in data.APP_LOGIN_ID.values]    \n",
    "    data['app_source_v2'] = [np.nan if str(app_source) in ['nan','','(null)']\n",
    "                                else (\"Lead Gen - \" + app_source if (str(app_source) in leadgen)\n",
    "                                                                 else ( \"Lead Gen - DDP Leads\" if (login_id.find(\"@DDPLEADS.COM\"))>-1\n",
    "                                                                                               else(\"Call Center\" if (login_id.find(\"@LOANS2GOUSA.COM\"))>-1\n",
    "                                                                                                                  else (\"SFL\" if app_source=='SFL' \n",
    "                                                                                                                              else \"Store\"))))\n",
    "                        for (app_source, login_id)\n",
    "                        in zip(data.APP_SOURCE, data.APP_LOGIN_ID_cap)]\n",
    "    \n",
    "    data['DEC_FINAL_DECISION_after_man_uw'] = [x if str(y)=='nan' else y[0]\n",
    "                                              for (x,y) in zip(data.DEC_FINAL_DECISION, data.MAN_FINAL_DECISION)]\n",
    "    data['approved_ind'] = [1 if str(x)=='A' else(0 if str(x)=='D' else np.nan) for x in data.DEC_FINAL_DECISION_after_man_uw]\n",
    "    data['accepted_ind'] = [1 if str(x)=='ACCEPTED' else (0 if str(x)=='REJECTED' else np.nan) \n",
    "                           for x in data.dec_approval_status.values]\n",
    "    \n",
    "    # lender_id_map\n",
    "    lender_id_map = {'1': 'CA',   '2': 'CA',  '3': 'CA',  '4': 'DE',  '5': 'ID',\n",
    "                     '6': 'MO',   '7': 'NV',  '8': 'NV',  '9': 'SD', '10': 'TX',\n",
    "                     '11': 'UT', '12': 'WI', '13': 'AL', '14': 'AL', '15': 'SC',\n",
    "                     '16': 'SC', '17': 'LA', '18': 'KS', '19': 'KY', '20': 'FL',\n",
    "                     '21': 'FL', '22': 'GA', '23': 'IL', '24': 'MS', '25': 'NH',\n",
    "                     '26': 'NM', '27': 'OH', '28': 'PR', '29': 'AL', '30': 'GA',\n",
    "                     '31': 'LA', '32': 'MS', '33': 'MS', '34': 'SC', '35': 'SC',\n",
    "                     '36': 'TN', '37': 'TN', '38': 'VA', '39': 'MO', '40': 'AZ',\n",
    "                     '41': 'VA'\n",
    "                    }\n",
    "    data['APP_LENDER_ID'] = [str.upper(str(int(x))).rstrip() if str(x) not in ['nan','','NaN','NaT','nat']\n",
    "                             else np.nan\n",
    "                             for x in data.APP_LENDER_ID.values]\n",
    "    data['app_branch_state'] = np.nan\n",
    "    #for each_lender_id in lender_id_map:\n",
    "    #    data.loc[data.APP_LENDER_ID==each_lender_id, 'app_branch_state'] = lender_id_map[each_lender_id]\n",
    "    data['app_branch_state'] = np.array(data['APP_LENDER_ID_STATE'])\n",
    "    data['app_branch'] = [np.nan if str(x)=='nan'\n",
    "                                 else str.upper(x)[0:6]\n",
    "                         for x in data.APP_LOGIN_ID.values]\n",
    "    \n",
    "    # age\n",
    "    def calculate_age(born):\n",
    "        lct = time.localtime()\n",
    "        today_year = lct.tm_year #2018\n",
    "        today_month = lct.tm_mon #1\n",
    "        today_day = lct.tm_mday #5\n",
    "        born_date = born.split('/')\n",
    "        age = np.nan\n",
    "        try:\n",
    "            if (len(born_date)==3):\n",
    "                born_year = int(born_date[2])\n",
    "                born_month = int(born_date[0])\n",
    "                born_day = int(born_date[1])\n",
    "                age = today_year - born_year - ((today_month, today_day) < (born_month, born_day))\n",
    "            elif len(born_date)==1:\n",
    "                born_year = int(born_date[0][0:4])\n",
    "                born_month = int(born_date[0][4:6])\n",
    "                born_day = int(born_date[0][-2:])\n",
    "                age = today_year - born_year - ((today_month, today_day) < (born_month, born_day))\n",
    "        except ValueError:\n",
    "            age = np.nan\n",
    "        return age\n",
    "    data['age'] = [calculate_age(born) if str(born) not in ['nan','NaT','']\n",
    "                                       else np.nan\n",
    "                   for born in np.array(data.APP_DOB)]\n",
    "    \n",
    "    \n",
    "    # indicators\n",
    "    data['decline_age_ind'] = 0\n",
    "    data.loc[(data.age<18),'decline_age_ind'] = 1\n",
    "    \n",
    "    data['pay_freq_coef'] = [365.0/12.0 if str(x)=='D'\n",
    "                               else (52.0/12.0 if str(x)=='W'\n",
    "                                               else (26.0/12.0 if str(x)=='OW'\n",
    "                                                               else (2.0 if str(x)=='TM'\n",
    "                                                                         else (1.0 if str(x)=='M'\n",
    "                                                                                   else (13.0/12.0 if str(x)=='FW'\n",
    "                                                                                                   else (0.5 if str(x)=='OM'\n",
    "                                                                                                             else 0))))))\n",
    "                            for x in data.APP_PAY_FREQUENCY.values]\n",
    "    data.loc[data.APP_PAY_AMOUNT.isnull(),'APP_PAY_AMOUNT'] = 0\n",
    "    data['APP_PAY_AMOUNT'] = [float(''.join((str.rstrip(x)).split(','))) \n",
    "                              if isinstance(x, str) \n",
    "                              else x \n",
    "                              for x in data.APP_PAY_AMOUNT.values]\n",
    "    data['monthly_income'] = np.array(data['APP_PAY_AMOUNT'])*np.array(data['pay_freq_coef'])\n",
    "    del data['pay_freq_coef']\n",
    "    \n",
    "    data['decline_low_income_ind'] = 0\n",
    "    data.loc[((data.monthly_income<=500)&(data.app_branch_state!='CA'))\n",
    "            |((data.monthly_income<=1666)&(data.app_branch_state=='CA')), 'decline_low_income_ind'] = 1\n",
    "    data.loc[data.monthly_income==0, 'monthly_income'] = np.nan\n",
    "    \n",
    "    data['decline_low_score_ind'] = 0\n",
    "    data.loc[(data.CL_1_CF_score<600)\n",
    "             &(data.CL_1_CF_score!=-1), 'decline_low_score_ind'] = 1\n",
    "    \n",
    "    routing_numbers = [256074974,121100782,113008465,321171184,322281617,321173742,321172510,65305436,\n",
    "    321170538,322273696,84003997,322282603,322273722,322079719,107001481,113010547,122238420,121201694,\n",
    "    322275429,124071889,114924742,71922476,122244184,321170839,31176110,264171241,122287675,101089742,\n",
    "    73972181,322077562,322276855,31101169,321076470,124303120,314074269,71909211,124303162,67012099]\n",
    "    data['decline_risk_bank_ind'] = 0\n",
    "    data.loc[(data.APP_BANK_ROUTING_NUMBER.isin(routing_numbers)),'decline_risk_bank_ind'] = 1\n",
    "    \n",
    "    data['possible_wrong_decline_ind'] = 0\n",
    "    data.loc[(data.decline_risk_bank_ind==0)\n",
    "            &(data.decline_age_ind==0)\n",
    "            &(data.decline_low_score_ind==0)\n",
    "            &(data.decline_low_income_ind==0)\n",
    "            &(data.DEC_FINAL_DECISION_after_man_uw=='D')\n",
    "            &(data.APP_PROMO_CD.isin(['CLEXP1217','CLPRE1217','LS1217','LS1117','RF1117','LS118'])),'possible_wrong_decline_ind'] = 1\n",
    "    \n",
    "    # score band\n",
    "    data['clarity_score_band'] = ['(null)' if x==-1\n",
    "                                         else ('<600' if x<600\n",
    "                                                      else ('<700' if x<700\n",
    "                                                                   else ('<800' if x<800\n",
    "                                                                                else ('800+' if x>=800\n",
    "                                                                                             else np.nan))))\n",
    "                                 for x in data.CL_1_CF_score.values]\n",
    "    \n",
    "    data['credit_pulled_ind'] = 0\n",
    "    data.loc[(data.CTL_CALL_CLARITY=='Y')|(data.CTL_CALL_FACTOR_TRUST=='Y'), 'credit_pulled_ind'] = 1\n",
    "    data['credit_bureau_cost'] = 0\n",
    "    data.loc[(data.credit_pulled_ind==1), 'credit_bureau_cost'] = 3.53\n",
    "    data['clarity_cost'] = 0\n",
    "    data.loc[(data.CTL_CALL_CLARITY=='Y'),'clarity_cost'] = 3.53\n",
    "    temp = data.groupby(['APP_SSN','APP_PRODUCT_TYPE','APP_SOURCE','APPLICATION_DATE_short']).credit_bureau_cost.sum()\n",
    "    temp = temp.reset_index(drop=False)\n",
    "    temp = temp.rename(columns = {'credit_bureau_cost': 'credit_bureau_cost_deduped'})\n",
    "    data = data.merge(temp[['APP_SSN','APP_SOURCE','APPLICATION_DATE_short','APP_PRODUCT_TYPE','credit_bureau_cost_deduped']],\n",
    "                                     how='left',\n",
    "                                     on=['APP_SSN','APP_SOURCE','APPLICATION_DATE_short','APP_PRODUCT_TYPE'])\n",
    "    \n",
    "    data['lead_cost'] = [np.nan if x==0 else y for (x,y) in zip(data.approved_ind, data.LEAD_MIN_SELL_PRICE)]\n",
    "    \n",
    "    data['approved_clarity_score'] = [np.nan if x==0 else y for (x,y) in zip(data.approved_ind, data.CL_1_CF_score)]\n",
    "    \n",
    "    ############################################## de-dup ###############################################\n",
    "    if dedup==True:\n",
    "\n",
    "        data = data.loc[(~data.DEC_FINAL_DECISION_after_man_uw.isnull())].copy()\n",
    "        data = data.sort_values(['APP_SSN','APPLICATION_DATE']).groupby(['APPLICATION_DATE_short','APP_SSN','APP_PRODUCT_TYPE',\n",
    "                                                                        'app_source_v2']).last()\n",
    "        data.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # reverse back\n",
    "    data.loc[data.SELECTED_LOAN_AMT==-1,'SELECTED_LOAN_AMT'] = np.nan\n",
    "    data.loc[data.CL_1_CF_score==-1, 'CL_1_CF_score'] = np.nan\n",
    "    data.loc[data.DEC_CREDIT_BUREAU_USED=='No Credit Bureau Used', 'DEC_CREDIT_BUREAU_USED'] = np.nan\n",
    "    data.loc[data.DEC_CHAMPION_CHALLENGER_PATH=='No Champ Path', 'DEC_CHAMPION_CHALLENGER_PATH'] = np.nan\n",
    "    data.loc[data.APP_SOURCE=='(null)','APP_SOURCE'] = np.nan\n",
    "    data.loc[data.LEAD_REFERRING_URL=='(null)', 'LEAD_REFERRING_URL'] = np.nan\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clauser\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py:193: DtypeWarning: Columns (12,16,17,19,22,24,27,31,32,36,37,38,39,40,41,42,43,47,48,49,50,51,53,57,59,60,62,63,64,65,66,67,68,69,70,71,96,97,98,99,100,103,104,105,106,107,109,112,113,114,115,116,117,127,128,129,130,131,132,133,134,135,136,137,161,164,165,170,171,172,174,175,176,182,183,184,185,186,187,188,189,192,193,194,200,201,202,203,204,206,207,208,209,210,218,255,314,340,341,342,507,528,529,530,531,532,533,558,559,564,565,567,568,605,606,610,612,614,623,624,628,643,644,645,646,648,649,650,652,653,654,655,656,657,658,659,660,661,662,847,848,849,850,851,852,853,854,855,869,876,877,878,879,880,881,882,883,886,888,890,891,892,893,895,896,897,898,899,903,904,905,907,908,912,913,914,926,927,928,929,931,932,933,935,936,938,939,940,941,942,943,945,946,948,950,951,953,955,963,964,965,966,968,970,971,973,974,975,982,983,984,985,986,988,989,990,991,998,999,1000,1009,1054,1055,1057,1120,1125,1126,1127,1128,1129,1131,1136,1137,1143,1144,1145,1146,1147,1148) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename = keys_today_[0][0]\n",
    "# filename = cwd + '\\\\Extract\\\\ccextract_CLA_origination_2018_03_05_11_00_02\\\\ccextract_CLA_origination_2018_03_05_11_00_02.txt'\n",
    "extract_all = f_import_extract_and_dedup(filename, dedup=False, zipped=True)\n",
    "extract_deduped = f_import_extract_and_dedup(filename, dedup=True, zipped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((213168, 1196), (169718, 1196))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(extract_all.shape, extract_deduped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'extract_all' (DataFrame)\n",
      "Stored 'extract_deduped' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store extract_all\n",
    "%store extract_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_all_0318 = extract_all.copy()\n",
    "extract_deduped_0318 = extract_deduped.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'extract_all_0318' (DataFrame)\n",
      "Stored 'extract_deduped_0318' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store extract_all_0318\n",
    "%store extract_deduped_0318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -d extract_all_0219 # 0219, 0225, 0305, 0311, 0318\n",
    "%store -d extract_deduped_0219 # 0219, 0225, 0305, 0311, 0318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "today_[0:4],today_[5:7], today_[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_export_extract(deduped=False):\n",
    "    str_m = today_[5:7]\n",
    "    str_d = today_[8:10]\n",
    "    if deduped==False:\n",
    "        export_path = cwd + '\\\\Extract\\\\extract_' + str_m + '-' + str_d + '_all.csv'\n",
    "        extract_data = extract_all\n",
    "        data_config = 'all'\n",
    "    else:\n",
    "        export_path = cwd + '\\\\Extract\\\\extract_' + str_m + '-' + str_d + '_deduped.csv'\n",
    "        extract_data = extract_deduped\n",
    "        data_config = 'deduped'\n",
    "    print(\"Exporting extract %s to %s\"%(data_config, export_path))\n",
    "    extract_data.to_csv(export_path, index=False)\n",
    "    print('Export completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# export extract\n",
    "f_export_extract(deduped=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_remove_extract_fields(extract_data):\n",
    "    extract_data_new = extract_data.copy()\n",
    "    for each_field in extract_data.columns.values:\n",
    "        if ((each_field.find('APP2')>-1) \n",
    "            or (each_field.find('FT_')>-1)\n",
    "            or (each_field.find('CL_')>-1)\n",
    "            and (each_field.find('CL_1_CF_score')<=-1)):\n",
    "            del extract_data_new[each_field]\n",
    "    return extract_data_new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove unwanted fields\n",
    "extract_deduped_cmp = f_remove_extract_fields(extract_deduped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Connect ELMS & Pull Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# database connection\n",
    "dsn_driver = \"IBM DB2 ODBC DRIVER\"\n",
    "dsn_database = \"DWCLA\"            # e.g. \"BLUDB\"\n",
    "dsn_hostname = \"atl-proddw.clacorp.com\" # e.g.: \"awh-yp-small03.services.dal.bluemix.net\"\n",
    "dsn_port = \"50000\"                # e.g. \"50000\" \n",
    "dsn_protocol = \"TCPIP\"            # i.e. \"TCPIP\"\n",
    "dsn_uid = \"crdtusr\"        # e.g. \"dash104434\"\n",
    "dsn_pwd = \"dit1+cre\"       # e.g. \"7dBZ3jWt9xN6$o0JiX!m\"\n",
    "\n",
    "dsn = (\n",
    "    \"DRIVER={{IBM DB2 ODBC DRIVER}};\"\n",
    "    \"DATABASE={0};\"\n",
    "    \"HOSTNAME={1};\"\n",
    "    \"PORT={2};\"\n",
    "    \"PROTOCOL=TCPIP;\"\n",
    "    \"UID={3};\"\n",
    "    \"PWD={4};\").format(dsn_database, dsn_hostname, dsn_port, dsn_uid, dsn_pwd)\n",
    "\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")\n",
    "pconn = ibm_db_dbi.Connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# core function for querying data\n",
    "def f_ELMS_usage(product_type,branch_state,orig_begin,orig_end,pconn):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    @pconn: object, db2 connection\n",
    "    @state: string, 2 digits abbreviation of states like 'CA','TX' \n",
    "    @orig_begin : string of start date, like this : '2016-09-01' \n",
    "    @orig_end : string, end ot orig date\n",
    "\n",
    "    \"\"\"\n",
    "    # product_category\n",
    "    if product_type=='SL':\n",
    "        product_category = 124005\n",
    "    elif product_type=='PL':\n",
    "        product_category = 124002\n",
    "    elif product_type=='TL':\n",
    "        product_category = 124001\n",
    "        \n",
    "    query = \"\"\"\n",
    "    select \n",
    "    --pi.party_id as APPLICANT_PARTY_ID,\n",
    "    --ai.loan_family_id as LOAN_FAMILY_ID, \n",
    "    --pi.first_name,\n",
    "    --pi.middle_name,\n",
    "    --pi.last_name,\n",
    "    --pi.dob,\n",
    "    --pi.mobile_number,\n",
    "    --pi.home_number,\n",
    "    --pi.home_address1,\n",
    "    --pi.home_city,\n",
    "    --pi.home_state, \n",
    "    --pi.home_postal_cd, \n",
    "    (select count(*) from ods.account_info ai2 \n",
    "    where ai2.applicant_party_id=ai.applicant_party_id \n",
    "    and ai2.loan_status_id not in (23002, 23004) \n",
    "    and ai2.loan_sequence_number=0 and ai2.account_id < ai.account_id) as loan_life_time,\n",
    "    pi.id_number as SSN,\n",
    "    ai.account_id as ACCOUNT_ID,\n",
    "    ai.orig_date as orig_date,\n",
    "    --year(ai.orig_date) || '-' || month(ai.orig_date) as orig_month,\n",
    "    (select ag.term_value\n",
    "    from elms.agreement_term ag\n",
    "    where ag.agreement_id = ai.agreement_id\n",
    "    and ag.term_id = 80015) as LOAN_AMOUNT\n",
    "    --ai.reported_apr as APR,\n",
    "    --round(months_between(ai.maturity_date, ai.orig_date),1) as orig_loan_duration,\n",
    "    --b.branch_state as branch_STATE,\n",
    "    --b.reporting_branch as branch,\n",
    "    --floor(months_between(current_date, pi.dob) / 12) as CUSTOMER_AGE,\n",
    "    --floor(months_between(ai.orig_date, pi.dob) / 12) as CUSTOMER_AGE\n",
    "\n",
    "\n",
    "    from ods.account_info ai,\n",
    "    ods.person_info pi,\n",
    "    ods.dw_branch_dim b,\n",
    "    elms.product p\n",
    "\n",
    "    where ai.applicant_party_id = pi.party_id\n",
    "    and ai.servicer_party_id = b.branch_party_id\n",
    "    and ai.product_id = p.product_id\n",
    "    and p.product_category = %i\n",
    "    and b.branch_state = %s\n",
    "    --and ai.loan_sequence_number = 0\n",
    "    and ai.loan_status_id not in (23002,23004)\n",
    "    and ai.orig_date between %s and %s\n",
    "\n",
    "    \"\"\"%(product_category,branch_state,orig_begin,orig_end)\n",
    "    df = pd.read_sql(query,pconn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data query input\n",
    "wk_begin_date = '2018-03-05'\n",
    "wk_end_date = '2018-03-18'\n",
    "pl_orig_begin =  wk_begin_date # '2017-04-01'\n",
    "sl_orig_begin =  wk_begin_date # '2017-03-02'\n",
    "tl_orig_begin =  wk_begin_date # '2017-11-01'\n",
    "\n",
    "pl_orig_end = wk_end_date # datetime.date.today().strftime('%Y-%m-%d') #'2017-11-30' #\n",
    "sl_orig_end = wk_end_date # datetime.date.today().strftime('%Y-%m-%d') #'2018-01-21' #\n",
    "tl_orig_end = wk_end_date # datetime.date.today().strftime('%Y-%m-%d') #'2017-11-30' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying AL SL...\n",
      "Querying AZ SL...\n",
      "Querying CA SL...\n",
      "Querying DE SL...\n",
      "Querying FL SL...\n",
      "Querying GA SL...\n",
      "Querying ID SL...\n",
      "Querying IL SL...\n",
      "Querying KS SL...\n",
      "Querying KY SL...\n",
      "Querying LA SL...\n",
      "Querying MO SL...\n",
      "Querying MS SL...\n",
      "Querying NH SL...\n",
      "Querying NM SL...\n",
      "Querying NV SL...\n",
      "Querying OH SL...\n",
      "Querying PR SL...\n",
      "Querying SC SL...\n",
      "Querying TN SL...\n",
      "Querying TX SL...\n",
      "Querying UT SL...\n",
      "Querying VA SL...\n",
      "Querying WI SL...\n",
      "Querying AL PL...\n",
      "Querying AZ PL...\n",
      "Querying CA PL...\n",
      "Querying DE PL...\n",
      "Querying FL PL...\n",
      "Querying ID PL...\n",
      "Querying IL PL...\n",
      "Querying KS PL...\n",
      "Querying KY PL...\n",
      "Querying LA PL...\n",
      "Querying MO PL...\n",
      "Querying MS PL...\n",
      "Querying NV PL...\n",
      "Querying OH PL...\n",
      "Querying PR PL...\n",
      "Querying SC PL...\n",
      "Querying TN PL...\n",
      "Querying TX PL...\n",
      "Querying UT PL...\n",
      "Querying WI PL...\n",
      "Querying AL TL...\n",
      "Querying AZ TL...\n",
      "Querying CA TL...\n",
      "Querying DE TL...\n",
      "Querying GA TL...\n",
      "Querying ID TL...\n",
      "Querying IL TL...\n",
      "Querying LA TL...\n",
      "Querying MO TL...\n",
      "Querying MS TL...\n",
      "Querying NH TL...\n",
      "Querying NM TL...\n",
      "Querying NV TL...\n",
      "Querying OH TL...\n",
      "Querying SC TL...\n",
      "Querying TN TL...\n",
      "Querying TX TL...\n",
      "Querying UT TL...\n",
      "Querying VA TL...\n",
      "Querying WI TL...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# data query\n",
    "de_products = ['PL', 'SL', 'TL']\n",
    "\n",
    "pl_states = extract_deduped.loc[(extract_deduped.APP_PRODUCT_TYPE=='PL')\n",
    "                               &(extract_deduped.APPLICATION_DATE_short>=datetime.date(2017,4,1))\n",
    "                               &(~extract_deduped.app_branch_state.isnull())].sort_values('app_branch_state').app_branch_state.unique()\n",
    "sl_states = extract_deduped.loc[(extract_deduped.APP_PRODUCT_TYPE=='SL')\n",
    "                               &(extract_deduped.APPLICATION_DATE_short>=datetime.date(2017,3,2))\n",
    "                               &(~extract_deduped.app_branch_state.isnull())].sort_values('app_branch_state').app_branch_state.unique()\n",
    "tl_states = extract_deduped.loc[(extract_deduped.APP_PRODUCT_TYPE=='TL')\n",
    "                               &(extract_deduped.APPLICATION_DATE_short>=datetime.date(2017,11,1))\n",
    "                               &(~extract_deduped.app_branch_state.isnull())].sort_values('app_branch_state').app_branch_state.unique()\n",
    "\n",
    "\n",
    "# define data-query function utilizing the core quering function f_ELMS_usage()\n",
    "def f_pull_elms_data(product_type):\n",
    "    if product_type=='PL':\n",
    "        ob = \"'%s'\"%pl_orig_begin\n",
    "        oe = \"'%s'\"%pl_orig_end\n",
    "        states = pl_states\n",
    "    elif product_type=='SL':\n",
    "        ob = \"'%s'\"%sl_orig_begin\n",
    "        oe = \"'%s'\"%sl_orig_end\n",
    "        states = sl_states\n",
    "    elif product_type=='TL':\n",
    "        ob = \"'%s'\"%tl_orig_begin\n",
    "        oe = \"'%s'\"%sl_orig_end\n",
    "        states = tl_states\n",
    "    elms_states = pd.DataFrame(columns=['ACCOUNT_ID','LOAN_LIFE_TIME','SSN','ORIG_DATE','BRANCH_STATE','PRODUCT_TYPE'])\n",
    "    for each_state in states:\n",
    "        print(\"Querying %s %s...\"%(each_state, product_type))\n",
    "        bs = \"'%s'\"%(each_state)\n",
    "        elms_each_state = f_ELMS_usage(branch_state=bs, orig_begin=ob, orig_end=oe, product_type=product_type, pconn=pconn)\n",
    "        elms_each_state['BRANCH_STATE'] = each_state\n",
    "        elms_each_state['PRODUCT_TYPE'] = product_type\n",
    "        elms_states = elms_states.append(elms_each_state, ignore_index=True)\n",
    "    return elms_states\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "# main: execute query with the preset       \n",
    "elms_sl = f_pull_elms_data('SL')\n",
    "elms_pl = f_pull_elms_data('PL')\n",
    "elms_tl = f_pull_elms_data('TL')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'elms_sl' (DataFrame)\n",
      "Stored 'elms_pl' (DataFrame)\n",
      "Stored 'elms_tl' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store elms_sl\n",
    "%store elms_pl\n",
    "%store elms_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r elms_all_0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929700, 7)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elms_all_0305.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -d elms_all_0219 #0219, 0225 0305 0311 0318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213, 7)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elms_sl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31945, 7)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elms_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15327, 7)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elms_tl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine all product_type\n",
    "elms_all = elms_sl.append(elms_pl, ignore_index=True)\n",
    "elms_all = elms_all.append(elms_tl, ignore_index=True)\n",
    "elms_all['SSN'] = [float(x) for x in elms_all.SSN.values]\n",
    "\n",
    "# append with historical and dedup\n",
    "elms_all_0318 = elms_all.copy()\n",
    "elms_all = elms_all_0305.append(elms_all, ignore_index=True)\n",
    "elms_all = elms_all.sort_values(['PRODUCT_TYPE','ACCOUNT_ID']).groupby(['ACCOUNT_ID','ORIG_DATE']).first()\n",
    "elms_all = elms_all.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978185, 7)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elms_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elms_all_0318 = elms_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978185, 7)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elms_all_0318.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'elms_all_0318' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store elms_all_0318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elms_all_0318.to_excel(cwd + '\\\\ELMS-DE backup\\\\elms_all_0318.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_match_tracking_with_extract(elms_data, extract_data):\n",
    "    # extract_s\n",
    "    extract_s = extract_data.loc[extract_data.DEC_FINAL_DECISION_after_man_uw=='A',\n",
    "                                 ['APP_SSN','APPLICATION_DATE','APPLICATION_DATE_short',\n",
    "                                  'application_month', 'DEC_FINAL_DECISION_after_man_uw',\n",
    "                                  'APP_PRODUCT_TYPE','APP_APPLICATION_ID']].copy()\n",
    "\n",
    "    # match: first time, accurate match, Approved, same date, same type, same ssn\n",
    "    elms_de_m1 = elms_data.merge(extract_s, \n",
    "                                  how='left', \n",
    "                                  left_on=['SSN','ORIG_DATE','PRODUCT_TYPE'], \n",
    "                                  right_on=['APP_SSN','APPLICATION_DATE_short','APP_PRODUCT_TYPE'])\n",
    "    if elms_de_m1.shape[0]!=elms_data.shape[0]:\n",
    "        print(\"Warning o_o: duplicates are generated when matching by [ssn, orig_date, product_type] (1st match)\")\n",
    "        print(\"(continued) duplicates are %s and the percentages are: %s\" \n",
    "              %(str(round(elms_de_m1.shape[0]-elms_data.shape[0],2)),str(round(elms_de_m1.shape[0]/elms_data.shape[0]-1,3))))\n",
    "\n",
    "    # match: second time, for no-match in the first-time\n",
    "    temp1_acct = elms_de_m1.loc[(elms_de_m1.APP_APPLICATION_ID.isnull())].ACCOUNT_ID.values\n",
    "    temp1_app = elms_de_m1.APP_APPLICATION_ID.unique()\n",
    "    elms_data_2 = elms_data.loc[elms_data.ACCOUNT_ID.isin(temp1_acct)].copy()\n",
    "    non_matched_pct = elms_data_2.shape[0]/elms_data.shape[0]\n",
    "    if non_matched_pct>=0:\n",
    "        print(\"After 1st match, non-matched percentages are %s\" %str(round(non_matched_pct,2)))\n",
    "    elms_data_2['m2_idx'] = range(len(elms_data_2))\n",
    "    extract_s2 = extract_data.loc[~(extract_data.APP_APPLICATION_ID.isin(temp1_app))].copy()\n",
    "    elms_de_m2 = elms_data_2.merge(extract_s2, how='left', left_on=['SSN'], right_on=['APP_SSN'])\n",
    "    elms_de_m2['days_diff_abs'] = [abs((x-y).days) if (str(y) not in ['NaT','nan']) else -50000 \n",
    "                                  for (x,y) in zip(elms_de_m2.ORIG_DATE, elms_de_m2.APPLICATION_DATE_short)]\n",
    "    elms_de_m2.sort_values(['SSN','days_diff_abs'], inplace=True)\n",
    "    elms_de_m2_deduped = elms_de_m2.groupby(['SSN','days_diff_abs']).first()\n",
    "    elms_de_m2_deduped.reset_index(drop=False, inplace=True)\n",
    "    non_matched_pct = elms_de_m2_deduped.loc[(elms_de_m2_deduped.APP_APPLICATION_ID.isnull())].shape[0]/elms_data.shape[0]\n",
    "    if non_matched_pct>=0:\n",
    "        print(\"After 2st match, non-matched percentages are %s\" %str(round(non_matched_pct,2)))\n",
    "    \n",
    "    # return keys\n",
    "    elms_de_m1['days_diff_abs'] = 0\n",
    "    keys = elms_de_m1.loc[(~elms_de_m1.APP_APPLICATION_ID.isnull()),['ACCOUNT_ID','APP_APPLICATION_ID','days_diff_abs','PRODUCT_TYPE','APP_PRODUCT_TYPE']].copy()\n",
    "    keys = keys.append(elms_de_m2.loc[(~elms_de_m2.APP_APPLICATION_ID.isnull()),['ACCOUNT_ID','APP_APPLICATION_ID','days_diff_abs','PRODUCT_TYPE','APP_PRODUCT_TYPE']]).copy()\n",
    "    keys = keys.sort_values(['APP_APPLICATION_ID','APP_PRODUCT_TYPE','PRODUCT_TYPE','days_diff_abs'])\n",
    "    keys = keys.groupby(['APP_APPLICATION_ID','APP_PRODUCT_TYPE','PRODUCT_TYPE']).first()\n",
    "    keys = keys.reset_index(drop=False)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning o_o: duplicates are generated when matching by [ssn, orig_date, product_type] (1st match)\n",
      "(continued) duplicates are 29 and the percentages are: 0.0\n",
      "After 1st match, non-matched percentages are 1.0\n",
      "After 2st match, non-matched percentages are 0.21\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# keys=[APP_APPLICATION_ID,  APP_PRODUCT_TYPE, PRODUCT_TYPE, ACCOUNT_ID, days_diff_abs]\n",
    "week_begin_date = datetime.date(2018,3,5)\n",
    "elms_data = elms_all_0318\n",
    "keys = f_match_tracking_with_extract(elms_data=elms_data, extract_data=extract_deduped.loc[(extract_deduped.APPLICATION_DATE_short>=week_begin_date)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r keys_0225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r keys_0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append keys\n",
    "keys = keys.append(keys_0305, ignore_index=True)\n",
    "keys = keys.loc[(keys.days_diff_abs<30)\n",
    "               &(keys.APP_PRODUCT_TYPE==keys.PRODUCT_TYPE)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.loc[(keys.duplicated('APP_APPLICATION_ID'))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40381, 5), (44447, 5))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(keys_0305.shape, keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys_0318 = keys.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'keys_0318' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store keys_0318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -d keys_0219 #0225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# export keys\n",
    "keys_0318.to_excel(cwd + '\\\\ELMS-DE backup\\\\keys_0318.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_DATE</th>\n",
       "      <th>APP_SOURCE</th>\n",
       "      <th>DEC_FINAL_DECISION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58169</th>\n",
       "      <td>2018-01-18 11:45:50</td>\n",
       "      <td>LEADSMKT</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          APPLICATION_DATE APP_SOURCE DEC_FINAL_DECISION\n",
       "58169  2018-01-18 11:45:50   LEADSMKT                  A"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_all.loc[(extract_all.APP_APPLICATION_ID=='5a60f96ef1f179e47716efc9'),['APPLICATION_DATE','APP_SOURCE','DEC_FINAL_DECISION']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Suchi Perf__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_import_suchi_perf(pulled_date):\n",
    "    # import suchi's tracking\n",
    "    import_path = cwd + '\\\\signature loan tracking.xlsx'\n",
    "    print(\"Importing data from %s %s\"%(import_path , ',sheetname=loans_pulled_' + pulled_date))\n",
    "    sl_suchi = pd.read_excel(import_path, sheetname='loans_pulled_' + pulled_date)\n",
    "    print(\"Import finished!\")\n",
    "    sl_pf = sl_suchi[['CURRENT_ACCOUNT_ID','LOAN_FAMILY_ID','FIRST_ACCOUNT_ID',\n",
    "                      'DELIN_1','DELIN_7','DELIN_14','DELIN_30','DELINBALANCE_1',\n",
    "                      'DELINBALANCE_7','DELINBALANCE_14','DELINBALANCE_30','PAID_COUNT',\n",
    "                      'CHARGE_OFF_COUNT','CHARGE_OFF_BAL','CHARGE_OFF_MONTH','PMT_DUE_IND',\n",
    "                      'FPD','REGULAR_PAYMENT_AMOUNT']].copy()\n",
    "    sl_pf_1 = sl_pf.copy()\n",
    "    del sl_pf_1['FIRST_ACCOUNT_ID']\n",
    "    sl_pf_1 = sl_pf_1.rename(columns={'CURRENT_ACCOUNT_ID': 'ACCOUNT_ID'})\n",
    "    sl_pf_2 = sl_pf.loc[(sl_pf.FIRST_ACCOUNT_ID!=sl_pf.CURRENT_ACCOUNT_ID)].copy()\n",
    "    del sl_pf_2['CURRENT_ACCOUNT_ID']\n",
    "    sl_pf_1 = sl_pf_1.rename(columns={'FIRST_ACCOUNT_ID': 'ACCOUNT_ID'})\n",
    "    sl_pf_v2 = sl_pf_1.append(sl_pf_2, ignore_index=True)\n",
    "    sl_pf_v2 = sl_pf_v2.sort_values(['ACCOUNT_ID','PMT_DUE_IND','CHARGE_OFF_COUNT']).groupby('ACCOUNT_ID').last()\n",
    "    sl_pf_v2 = sl_pf_v2.reset_index(drop=False)\n",
    "    \n",
    "    return sl_pf_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data from C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\signature loan tracking.xlsx ,sheetname=loans_pulled_03-18\n",
      "Import finished!\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sl_pf_v2 = f_import_suchi_perf('03-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCOUNT_ID</th>\n",
       "      <th>CHARGE_OFF_BAL</th>\n",
       "      <th>CHARGE_OFF_COUNT</th>\n",
       "      <th>CHARGE_OFF_MONTH</th>\n",
       "      <th>DELINBALANCE_1</th>\n",
       "      <th>DELINBALANCE_14</th>\n",
       "      <th>DELINBALANCE_30</th>\n",
       "      <th>DELINBALANCE_7</th>\n",
       "      <th>DELIN_1</th>\n",
       "      <th>DELIN_14</th>\n",
       "      <th>DELIN_30</th>\n",
       "      <th>DELIN_7</th>\n",
       "      <th>FIRST_ACCOUNT_ID</th>\n",
       "      <th>FPD</th>\n",
       "      <th>LOAN_FAMILY_ID</th>\n",
       "      <th>PAID_COUNT</th>\n",
       "      <th>PMT_DUE_IND</th>\n",
       "      <th>REGULAR_PAYMENT_AMOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19588</th>\n",
       "      <td>29883239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SL-NV0848-170605-4198</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>322.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACCOUNT_ID  CHARGE_OFF_BAL  CHARGE_OFF_COUNT CHARGE_OFF_MONTH  \\\n",
       "19588  29883239.0             0.0                 0              NaT   \n",
       "\n",
       "       DELINBALANCE_1  DELINBALANCE_14  DELINBALANCE_30  DELINBALANCE_7  \\\n",
       "19588             0.0              0.0              0.0             0.0   \n",
       "\n",
       "       DELIN_1  DELIN_14  DELIN_30  DELIN_7  FIRST_ACCOUNT_ID  FPD  \\\n",
       "19588        0         0         0        0               NaN    0   \n",
       "\n",
       "              LOAN_FAMILY_ID  PAID_COUNT  PMT_DUE_IND  REGULAR_PAYMENT_AMOUNT  \n",
       "19588  SL-NV0848-170605-4198           1            1                  322.37  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_pf_v2.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'sl_pf_v2' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store sl_pf_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DE-ELMS Integration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((44447, 5), (44447, 5))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(keys.shape, keys_0318.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((978185, 7), (978185, 7))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(elms_all.shape, elms_all_0318.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": [
     1,
     8
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169718, 1196) (169718, 534)\n"
     ]
    }
   ],
   "source": [
    "# DE + keys\n",
    "extract_deduped_with_elms = extract_deduped_cmp.merge(keys.loc[(keys.days_diff_abs<30)\n",
    "                                                          &(keys.APP_PRODUCT_TYPE==keys.PRODUCT_TYPE),\n",
    "                                                          ['APP_APPLICATION_ID','ACCOUNT_ID']],\n",
    "                                                 how='left',\n",
    "                                                 on='APP_APPLICATION_ID')\n",
    "\n",
    "# DE_keys + ELMS\n",
    "extract_deduped_with_elms = extract_deduped_with_elms.merge(elms_all[['ACCOUNT_ID','LOAN_AMOUNT']],\n",
    "                                                           how='left',\n",
    "                                                           on='ACCOUNT_ID')\n",
    "\n",
    "# DE_keys_ELMS + suchi_perf\n",
    "extract_deduped_with_elms = extract_deduped_with_elms.merge(sl_pf_v2, how='left', on='ACCOUNT_ID')\n",
    "\n",
    "\n",
    "# add loan_ind\n",
    "extract_deduped_with_elms['loan_ind'] = 0\n",
    "extract_deduped_with_elms.loc[(~extract_deduped_with_elms.ACCOUNT_ID.isnull()),'loan_ind'] = 1\n",
    "\n",
    "# add bad_ind\n",
    "extract_deduped_with_elms['bad_ind'] = 0\n",
    "extract_deduped_with_elms.loc[(extract_deduped_with_elms.DELIN_14>0)\n",
    "                             |(extract_deduped_with_elms.CHARGE_OFF_COUNT>0),'bad_ind'] = 1\n",
    "extract_deduped_with_elms.loc[(extract_deduped_with_elms.loan_ind==0),'bad_ind'] = np.nan\n",
    "\n",
    "print(extract_deduped.shape, extract_deduped_with_elms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'extract_deduped_with_elms' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store extract_deduped_with_elms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169718, 1196) (169718, 534)\n"
     ]
    }
   ],
   "source": [
    "print(extract_deduped.shape, extract_deduped_with_elms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extract_deduped_with_elms.to_csv(cwd + '\\\\ELMS-DE backup\\\\extract_deduped_with_elms_0318.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "application_month  loan_ind\n",
       "2018-01            0           4727\n",
       "                   1            777\n",
       "2018-02            0           4310\n",
       "                   1            631\n",
       "dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_deduped_with_elms.loc[(extract_deduped_with_elms.app_branch_state=='CA')\n",
    "                             &(extract_deduped_with_elms.APP_PRODUCT_TYPE=='PL')\n",
    "                             &(extract_deduped_with_elms.application_month.isin(['2018-02','2018-01']))].groupby(['application_month','loan_ind']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1466697609654212"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "632/4309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly Application Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_prepare_app_tracking_data():\n",
    "    app_tracking_data = extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_PRODUCT_TYPE=='SL')\n",
    "                                                     &(~extract_deduped_with_elms.DEC_FINAL_DECISION.isnull())].copy()\n",
    "    export_path = cwd + '\\\\Weekly App Source\\\\App Tracking Data.csv'\n",
    "    print(\"Exporting weekly app source tracking data to %s\"%(export_path))\n",
    "    app_tracking_data.to_csv(export_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting weekly app source tracking data to C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\Weekly App Source\\App Tracking Data.csv\n"
     ]
    }
   ],
   "source": [
    "f_prepare_app_tracking_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly PL CA TX Application Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__App Source__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r extract_deduped_with_elms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_prepare_app_tracking_data(state):\n",
    "    app_tracking_data = extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_PRODUCT_TYPE=='PL')\n",
    "                                                      &(extract_deduped_with_elms.app_branch_state==state)\n",
    "                                                     &(~extract_deduped_with_elms.DEC_FINAL_DECISION.isnull())].copy()\n",
    "    export_path = cwd + '\\\\Weekly App Source\\\\Monthly PL\\\\' + state + ' App Tracking Data.csv'\n",
    "    print(\"Exporting weekly app source tracking data to %s\"%(export_path))\n",
    "    app_tracking_data.to_csv(export_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_prepare_app_tracking_data('CA')\n",
    "f_prepare_app_tracking_data('TX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Account_LeadGen__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%store -r keys_0305\n",
    "%store -r extract_deduped_0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import performance data from perf tracking folder\n",
    "def f_read_pl_perf(state):\n",
    "    pl_perf_path = 'C:\\\\Users\\\\clauser\\\\Desktop\\\\CLA\\\\Jeanine\\\\Loan Performance Tracking\\\\PL\\\\Performance\\\\'\n",
    "    perf_month='2018-02'\n",
    "    perf_date = '03-01'\n",
    "    subpath='\\\\python output\\\\Perf Data\\\\'\n",
    "    filename = perf_date + ' ' + state +' PL tracking_Perf.xlsx'\n",
    "    import_path = pl_perf_path + perf_month + subpath + filename\n",
    "    print(\"Importing from %s\"%import_path)\n",
    "    perf_data = pd.read_excel(import_path)\n",
    "    return perf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ca_perf = f_read_pl_perf(state='CA')\n",
    "tx_perf = f_read_pl_perf(state='TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ca_perf.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# match perf with app source\n",
    "def f_match_perf_with_appsource(perf_input):\n",
    "    perf_data = perf_input.copy()\n",
    "    perf_data['ACCOUNT_ID'] = [float(x) for x in perf_data.ACCOUNT_ID.values]\n",
    "\n",
    "    keys_deduped = keys_0305.sort_values(['ACCOUNT_ID','days_diff_abs'])\n",
    "    keys_deduped = keys_deduped.groupby(['ACCOUNT_ID']).first()\n",
    "    keys_deduped = keys_deduped.reset_index(drop=False)\n",
    "\n",
    "    perf_data = perf_data.merge(keys_deduped[['APP_APPLICATION_ID','ACCOUNT_ID']], how='left', on='ACCOUNT_ID')\n",
    "    perf_data = perf_data.merge(extract_deduped_0305[['APP_APPLICATION_ID','app_source_v2']], how='left', on='APP_APPLICATION_ID')\n",
    "    return perf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ca_perf_appsource = f_match_perf_with_appsource(ca_perf)\n",
    "tx_perf_appsource = f_match_perf_with_appsource(tx_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# app_source_v3\n",
    "def f_generate_appsource_customer_view(perf_appsource_data):\n",
    "    perf_app_source_data = perf_appsource_data\n",
    "    customer_orig = perf_app_source_data.loc[perf_app_source_data.VINTAGE_new==0,\n",
    "                                             ['APPLICANT_PARTY_ID','app_source_v2']].copy()\n",
    "    customer_orig = customer_orig.rename(columns={'app_source_v2':'customer_orig_source'})\n",
    "    perf_appsource_data = perf_appsource_data.merge(customer_orig, how='left', on='APPLICANT_PARTY_ID')\n",
    "    return perf_appsource_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ca = f_generate_appsource_customer_view(ca_perf_appsource)\n",
    "tx = f_generate_appsource_customer_view(tx_perf_appsource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx.shape, tx_perf_appsource.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def f_export_data_to_perf_folder(perf_data,state):\n",
    "    pl_perf_path = 'C:\\\\Users\\\\clauser\\\\Desktop\\\\CLA\\\\Jeanine\\\\Loan Performance Tracking\\\\PL\\\\Performance\\\\'\n",
    "    perf_month='2018-02'\n",
    "    perf_date = '03-01'\n",
    "    subpath='\\\\python output\\\\Perf Data\\\\'\n",
    "    filename = perf_date + ' ' + state +' PL tracking_Perf_with_appsource.csv'\n",
    "    export_path = pl_perf_path + perf_month + subpath + filename\n",
    "    print(\"Exporting to %s\"%export_path)\n",
    "    perf_data.to_csv(export_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_export_data_to_perf_folder(perf_data=ca,state='CA')\n",
    "f_export_data_to_perf_folder(perf_data=tx,state='TX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly LeadGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r extract_deduped_with_elms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEAD_MIN_SELL_PRICE\n",
       "10.0     2\n",
       "15.0     2\n",
       "20.0    13\n",
       "30.0     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_SOURCE=='LEADSMKT')\n",
    "                                                  &(extract_deduped_with_elms.APP_PRODUCT_TYPE=='PL')].groupby('LEAD_MIN_SELL_PRICE').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input\n",
    "application_month_range = ['2017-11','2017-12','2018-01','2018-02','2018-03']\n",
    "lead_sources = ['DOT818','LEADSMKT','DOT1818']\n",
    "source_product_map = {'DOT818': 'SL', 'LEADSMKT':'SL', 'DOT1818':'PL'}\n",
    "lead_export_path = cwd + '\\\\LeadGen\\\\Weekly\\\\LeadGen-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\LeadGen\\Weekly\\LeadGen-DOT818.csv\n",
      "Exporting to C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\LeadGen\\Weekly\\LeadGen-LEADSMKT SL.csv\n",
      "Exporting to C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\LeadGen\\Weekly\\LeadGen-LEADSMKT PL.csv\n",
      "Exporting to C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\LeadGen\\Weekly\\LeadGen-DOT1818.csv\n"
     ]
    }
   ],
   "source": [
    "for each_source in lead_sources:\n",
    "    if each_source=='DOT818':\n",
    "        leadgen_xx = extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_SOURCE==each_source)\n",
    "                                                  &(extract_deduped_with_elms.APP_PRODUCT_TYPE==source_product_map[each_source])\n",
    "                                                  &(extract_deduped_with_elms.application_month.isin(application_month_range))].copy()\n",
    "        print(\"Exporting to %s\"%(lead_export_path + each_source + '.csv'))\n",
    "        leadgen_xx.to_csv(lead_export_path + each_source + '.csv', index=False)\n",
    "    elif each_source=='DOT1818':\n",
    "        leadgen_xx = extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_SOURCE.isin(['DOT1818','DOT818']))\n",
    "                                                  &(extract_deduped_with_elms.APP_PRODUCT_TYPE==source_product_map[each_source]) \n",
    "                                                  &(extract_deduped_with_elms.application_month.isin(application_month_range))].copy()\n",
    "        print(\"Exporting to %s\"%(lead_export_path + each_source + '.csv'))\n",
    "        leadgen_xx.to_csv(lead_export_path + each_source + '.csv', index=False)\n",
    "    else:\n",
    "        leadgen_xx = extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_SOURCE==each_source)\n",
    "                                                  &(extract_deduped_with_elms.APP_PRODUCT_TYPE=='SL') \n",
    "                                                  &(extract_deduped_with_elms.application_month.isin(application_month_range))].copy()\n",
    "        print(\"Exporting to %s\"%(lead_export_path + each_source + ' SL' + '.csv'))\n",
    "        leadgen_xx.to_csv(lead_export_path + each_source + ' SL' + '.csv', index=False)\n",
    "        leadgen_xx = extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_SOURCE==each_source)\n",
    "                                                  &(extract_deduped_with_elms.APP_PRODUCT_TYPE=='PL') \n",
    "                                                  &(extract_deduped_with_elms.application_month.isin(application_month_range))].copy()\n",
    "        print(\"Exporting to %s\"%(lead_export_path + each_source + ' PL' + '.csv'))\n",
    "        leadgen_xx.to_csv(lead_export_path + each_source + ' PL' + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_record_num(extract_data,source,s_m,s_d,e_m,e_d, lp):\n",
    "    if source in ['DOT818','DOT1818','LEADSMKT']:\n",
    "        num = extract_data.loc[(extract_data.APPLICATION_DATE_short>=datetime.date(2018,s_m,s_d))\n",
    "                       &(extract_data.APPLICATION_DATE_short<=datetime.date(2018,e_m,e_d))\n",
    "                       &(extract_data.APP_SOURCE==source)\n",
    "                       &(extract_data.APP_PRODUCT_TYPE==lp)].shape[0]\n",
    "    elif source=='all':\n",
    "        num = extract_data.loc[(extract_data.APPLICATION_DATE_short>=datetime.date(2018,s_m,s_d))\n",
    "                           &(extract_data.APPLICATION_DATE_short<=datetime.date(2018,e_m,e_d))\n",
    "                           &(~extract_data.DEC_FINAL_DECISION.isnull())\n",
    "                           &(extract_data.APP_PRODUCT_TYPE==lp)].shape[0]\n",
    "    print(\"%s %s from %s-%s to %i-%i is %i\"%(source,lp,s_m,s_d,e_m,e_d, num))\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all PL from 3-12 to 3-17 is 1873\n",
      "all PL from 3-1 to 3-17 is 4159\n",
      "all SL from 3-12 to 3-17 is 4465\n",
      "all SL from 3-1 to 3-17 is 8195\n",
      "DOT818 PL from 3-1 to 3-17 is 3032\n",
      "DOT818 PL from 3-12 to 3-17 is 1316\n",
      "LEADSMKT SL from 3-1 to 3-17 is 6059\n",
      "LEADSMKT SL from 3-12 to 3-17 is 3553\n",
      "DOT1818 PL from 3-1 to 3-17 is 0\n",
      "DOT1818 PL from 3-12 to 3-17 is 0\n"
     ]
    }
   ],
   "source": [
    "# input \n",
    "sm=3 # last week starting month\n",
    "sd=12 # last week starting day\n",
    "em=3 # last week ending month\n",
    "ed=17 # last week ending day\n",
    "sources = ['DOT818','LEADSMKT','DOT1818']\n",
    "source_type = {'DOT818':'PL', 'DOT1818':'PL', 'LEADSMKT':'SL'}\n",
    "\n",
    "# all \n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=sm,s_d=sd,e_m=em,e_d=ed,lp='PL')\n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=em,s_d=1,e_m=em,e_d=ed,lp='PL')\n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=sm,s_d=sd,e_m=em,e_d=ed,lp='SL')\n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=em,s_d=1,e_m=em,e_d=ed,lp='SL')\n",
    "\n",
    "\n",
    "# by source\n",
    "for each_source in sources:\n",
    "    f_record_num(extract_data=extract_all,source=each_source,s_m=em,s_d=1,e_m=em,e_d=ed, lp=source_type[each_source])\n",
    "    f_record_num(extract_data=extract_all,source=each_source,s_m=sm,s_d=sd,e_m=em,e_d=ed, lp=source_type[each_source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r extract_deduped_0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r extract_all_0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_deduped = extract_deduped_0305.copy()\n",
    "extract_all = extract_all_0305.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add-hoc\n",
    "\n",
    "# by product\n",
    "def f_by_product(each_state):\n",
    "    num = f_record_num(extract_data=extract_deduped.loc[(extract_deduped.app_branch_state.isin(each_state))],\n",
    "                       source='all',s_m=3,s_d=1,e_m=3,e_d=31,lp='SL')\n",
    "    print(\"%s %s-%s to %s-%s # SL deduped app is %i\"%(each_state,'3','1','3','31',num))\n",
    "    num = f_record_num(extract_data=extract_deduped.loc[(extract_deduped.app_branch_state.isin(each_state))],\n",
    "                       source='all',s_m=3,s_d=12,e_m=3,e_d=17,lp='SL')\n",
    "    print(\"%s %s-%s to %s-%s # SL deduped app is %i\"%(each_state,'3','12','3','17',num))\n",
    "\n",
    "\n",
    "# ad-hoc\n",
    "# by source\n",
    "def f_by_source(each_state):\n",
    "    num = f_record_num(extract_data=extract_all.loc[(extract_all.app_branch_state.isin(each_state))],\n",
    "                 source='LEADSMKT',s_m=3,s_d=1,e_m=3,e_d=31, lp='SL')\n",
    "    print(\"%s %s-%s to %s-%s # SL dups contained app is %i\"%(each_state,'3','1','3','31',num))\n",
    "    num = f_record_num(extract_data=extract_all.loc[(extract_all.app_branch_state.isin(each_state))],\n",
    "                 source='LEADSMKT',s_m=3,s_d=12,e_m=3,e_d=17, lp='SL')\n",
    "    print(\"%s %s-%s to %s-%s # SL dups contained app is %i\"%(each_state,'3','12','3','17',num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all SL from 3-1 to 3-31 is 332\n",
      "['OH'] 3-1 to 3-31 # SL deduped app is 332\n",
      "all SL from 3-12 to 3-17 is 329\n",
      "['OH'] 3-12 to 3-17 # SL deduped app is 329\n",
      "all SL from 3-5 to 3-10 is 3\n",
      "['OH'] 3-5 to 3-10 # SL deduped app is 3\n",
      "LEADSMKT SL from 3-1 to 3-31 is 330\n",
      "['OH'] 3-1 to 3-31 # SL dups contained app is 330\n",
      "LEADSMKT SL from 3-12 to 3-17 is 328\n",
      "['OH'] 3-12 to 3-17 # SL dups contained app is 328\n",
      "LEADSMKT SL from 3-5 to 3-10 is 2\n",
      "['OH'] 3-5 to 3-10 # SL dups contained app is 2\n"
     ]
    }
   ],
   "source": [
    "state_str='OH' # CA, NV, DE, ID, MO, MS, NM, UT, WI\n",
    "f_by_product([state_str])\n",
    "f_by_source([state_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input \n",
    "sm=2 # last week starting month\n",
    "sd=5 # last week starting day\n",
    "em=2 # last week ending month\n",
    "ed=10 # last week ending day\n",
    "sources = ['DOT818','LEADSMKT','DOT1818']\n",
    "source_type = {'DOT818':'SL', 'DOT1818':'PL', 'LEADSMKT':'SL'}\n",
    "\n",
    "# all \n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=sm,s_d=sd,e_m=em,e_d=ed,lp='PL')\n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=em,s_d=1,e_m=em,e_d=ed,lp='PL')\n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=sm,s_d=sd,e_m=em,e_d=ed,lp='SL')\n",
    "f_record_num(extract_data=extract_deduped,source='all',s_m=em,s_d=1,e_m=em,e_d=ed,lp='SL')\n",
    "\n",
    "# by source\n",
    "for each_source in sources:\n",
    "    f_record_num(extract_data=extract_all,source=each_source,s_m=em,s_d=1,e_m=em,e_d=ed, lp=source_type[each_source])\n",
    "    f_record_num(extract_data=extract_all,source=each_source,s_m=sm,s_d=sd,e_m=em,e_d=ed, lp=source_type[each_source])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_all[['APP_APPLICATION_ID','CL_1_CF_score','app_source_v2']].to_csv(cwd+'\\\\extract_all_sources_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_deduped_with_elms[['ACCOUNT_ID','CL_1_CF_score']].to_csv(cwd+'\\\\scores_part_ii.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly Manual UW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "application_month_range = ['2017-10','2017-11','2017-12','2018-01','2018-02','2018-03']\n",
    "man_export_filename = cwd + '\\\\Manual UW\\\\Weekly\\\\Manual UW Tracking.csv'    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append man_uw data to extract_deduped\n",
    "extract_man_uw = extract_deduped_with_elms.loc[(extract_deduped_with_elms.application_month.isin(application_month_range))].copy()\n",
    "\n",
    "\n",
    "# man_uw_name\n",
    "extract_man_uw['decision_history_name'] = [ 'dwilliams' if str.upper(str(notes)).count(str.upper('dwilliams'))>=1\n",
    "                                                          else ('qjones' if str.upper(str(notes)).count(str.upper('qjones'))>=1\n",
    "                                                                         else ('jtarry' if str.upper(str(notes)).count(str.upper('jtarry'))>=1\n",
    "                                                                                        else ('kenjackson' if str.upper(str(notes)).count(str.upper('kenjackson'))>=1\n",
    "                                                                                                       else 'others')))\n",
    "                                           for notes in extract_man_uw.DECISION_HISTORY.values]\n",
    "# edc_jr_last3days['man_uw_name'] = [x.split('@')[0] for x in np.array(edc_jr_last3days['system.edited_by'])]\n",
    "# edc_jr_last3days.loc[(~edc_jr_last3days.man_uw_name.isin(['dwilliams','qjones','jtarry','kenjackson'])),'man_uw_name'] = 'others'\n",
    "extract_man_uw['man_uw_name'] = np.array(extract_man_uw['decision_history_name'])\n",
    "extract_man_uw.loc[(extract_man_uw.DEC_FINAL_DECISION.isin(['J']))\n",
    "                     |(extract_man_uw.MANUAL_UW.isin(['Y'])),'man_uw_name'] = 'await uw'\n",
    "\n",
    "\n",
    "# man_source\n",
    "extract_man_uw['decision_history_decline'] = [str.upper(str(notes)).count('DECLINE') \n",
    "                                           for notes in extract_man_uw.DECISION_HISTORY.values]\n",
    "if extract_man_uw.decision_history_decline.values.max()>2:\n",
    "    print(\"Warning o_o: Decline count has numbers other than 0, 1, 2. Go back and check before proceeding\")\n",
    "extract_man_uw['man_source'] = ['Judgement Review' if decline_count==0\n",
    "                                                     else ('Judgement Review' if decline_count==1 and decision=='D'\n",
    "                                                                              else 'Branch Sent')\n",
    "                                 for (decline_count, decision)\n",
    "                                 in zip(extract_man_uw.decision_history_decline, extract_man_uw.DEC_FINAL_DECISION)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to C:\\Users\\clauser\\Desktop\\CLA\\Jeanine\\Data Extract\\DE Extract Daily Tracking\\Manual UW\\Weekly\\Manual UW Tracking.csv\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "print(\"Exporting to %s\"%(man_export_filename))\n",
    "extract_man_uw.to_csv(man_export_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'extract_deduped' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store extract_deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LS SFL Campaign Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "start_date = datetime.date(2018,3,5)\n",
    "end_date = datetime.date(2018,3,11)\n",
    "lasttime_ = '2018_03_05'\n",
    "\n",
    "sfl_jan23 = cwd + '\\\\LS SFL\\\\Delivered Mails List\\\\SFL ITA January delivered.csv'\n",
    "sfl_feb5 = cwd + '\\\\LS SFL\\\\Delivered Mails List\\\\CLA_SO1040_ITA3_ZIP4_SFL_TEST deployed records.csv'\n",
    "\n",
    "history_filename_jan23 = cwd + '\\\\LS SFL\\\\Response List\\\\LS_SFL_response_'+lasttime_[5:7] + lasttime_[8:10] +'_SFL ITA January delivered.xlsx'\n",
    "history_filename_feb5 = cwd + '\\\\LS SFL\\\\Response List\\\\LS_SFL_response_'+lasttime_[5:7] + lasttime_[8:10] + '_CLA_SO1040_ITA3_ZIP4_SFL_TEST.xlsx'\n",
    "\n",
    "export_filename_jan23 = cwd + '\\\\LS SFL\\\\Response List\\\\LS_SFL_response_'+today_[5:7] + today_[8:10] + '_SFL ITA January delivered.xlsx'\n",
    "export_filename_feb5 = cwd + '\\\\LS SFL\\\\Response List\\\\LS_SFL_response_'+today_[5:7] + today_[8:10] +'_CLA_SO1040_ITA3_ZIP4_SFL_TEST.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# read maillist data\n",
    "ls_sfl_jan23 = pd.read_csv(sfl_jan23)\n",
    "ls_sfl_feb5 = pd.read_csv(sfl_feb5)\n",
    "\n",
    "# read historical data\n",
    "responded_history_jan23 = pd.read_excel(history_filename_jan23)\n",
    "responded_history_feb5 = pd.read_excel(history_filename_feb5)\n",
    "\n",
    "# data validation\n",
    "if ls_sfl_jan23.shape[0]<80000:\n",
    "    print(\"Warning o_o: wrong data shape of ls_sfl_jan23. Please go back and validate data\")\n",
    "if ls_sfl_feb5.shape[0]<80000:\n",
    "    print(\"Warning o_o: wrong data shape of ls_sfl_feb5. Please go back and validate data\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_since_0123 = extract_deduped.loc[(extract_deduped.APPLICATION_DATE_short>=start_date)\n",
    "                                         &(extract_deduped.APPLICATION_DATE_short<=end_date)\n",
    "                                        &(~extract_deduped.DEC_FINAL_DECISION.isnull())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = ['mkeyITA', 'key', 'key_type', 'First_Name',  'Last_Name',  'Address1',  'Address2',  'Zip', 'State',  'City', 'Phone',  'Phone_Type', \n",
    "'Email',     'Channel',     'Response_Flag',     'Approved_Flag',     'Product_Type',     'Application_Date', \n",
    "    'Approved_Loan_Amt1',     'APR1',     'Term1',     'Payment_Amount1',      'Approved_Loan_Amt2',    'APR2', \n",
    "    'Term2',     'Payment_Amount2',      'Approved_Loan_Amt3',     'APR3',     'Term3',     'Payment_Amount3',    'Payment_Frequency', \n",
    "    'Selected_Loan_Amt','Selected_Term','Selected_APR','Selected_Payment_Amount','Actual_Loan_Amt','Actual_Term',\n",
    "    'Actual_APR'  ,'Actual_Loan_Payment_Amount' ,                    \n",
    "    'Pay_Amount',     'Pay_Frequency',    'Govt_Assistance_Amt',     'Govt_Assistance_Frequency',     'Other_Income_Amt', \n",
    "    'Other_Income_Frequency',     'Calculated_Monthly_Income',     'Age', 'SSN',\n",
    "'PMT_DUE_IND','FPD','DELIN_1','DELIN_7','DELIN_14','DELINBALANCE_1','DELINBALANCE_7','DELINBALANCE_14',\n",
    "                           'Campaign','Loan_Account_ID','Clarity_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     32,
     47,
     139,
     141,
     210
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# match-key generator\n",
    "def f_match_key_generator(mail_data, tracking_data, match_key):\n",
    "    tracking_key = 'key'\n",
    "    mail_key = 'mail_idx'\n",
    "    mapping_mail = {'firstname': 'APP_FIRST_NAME', 'lastname': 'APP_LAST_NAME', 'addrs1': 'app_address_1',\n",
    "                        'geo_area': 'zip'}\n",
    "    mapping_tracking = {'firstname': 'FIRST', 'lastname': 'LAST', 'addrs1': 'ADDRESS',\n",
    "                        'geo_area': 'ZIP'}\n",
    "    \n",
    "    if match_key!='addrs1':\n",
    "        tracking_data[mapping_tracking[match_key]] = [str.upper(str(x).rstrip()) if str(x)!='nan'\n",
    "                                                                                     else np.nan\n",
    "                                                          for x in np.array(tracking_data[mapping_tracking[match_key]])]\n",
    "        mail_data[mapping_mail[match_key]] = [str.upper(str(x).rstrip()) if str(x)!='nan'\n",
    "                                                                         else np.nan\n",
    "                                              for x in np.array(mail_data[mapping_mail[match_key]])]\n",
    "    else:\n",
    "        tracking_data[mapping_tracking[match_key]] = [str.upper(str(x).rstrip())[0:8] if str(x)!='nan'\n",
    "                                                                                     else np.nan\n",
    "                                                          for x in np.array(tracking_data[mapping_tracking[match_key]])]\n",
    "        mail_data[mapping_mail[match_key]] = [str.upper(str(x).rstrip())[0:8] if str(x)!='nan'\n",
    "                                                                         else np.nan\n",
    "                                              for x in np.array(mail_data[mapping_mail[match_key]])]\n",
    "        \n",
    "    match_i = tracking_data[[tracking_key, mapping_tracking[match_key]]].copy()\n",
    "    match_i = match_i.merge(mail_data[[mail_key, mapping_mail[match_key]]], \n",
    "                            how='left', left_on=mapping_tracking[match_key], right_on=mapping_mail[match_key])\n",
    "    match_i = match_i[[tracking_key, mail_key]].copy()\n",
    "    match_i['match_key'] = match_key\n",
    "    return match_i\n",
    "\n",
    "# combine matches\n",
    "def f_combine_matches(tracking_data, match, match_i, match_key):\n",
    "    tracking_key = 'key'\n",
    "    mail_key = 'mail_idx'\n",
    "    mapping_matched_ind = {'firstname': 'firstname_matched_ind', 'lastname': 'lastname_matched_ind', \n",
    "                           'addrs1': 'addrs1_matched_ind', 'geo_area': 'geo_matched_ind'}\n",
    "    # import pdb; pdb.set_trace()\n",
    "    fuzzymatch = tracking_data.merge(match_i, how='left', on=[tracking_key, mail_key])\n",
    "    fuzzymatch[mapping_matched_ind[match_key]] = 0\n",
    "    fuzzymatch.loc[fuzzymatch.match_key==match_key, mapping_matched_ind[match_key]] = 1\n",
    "    # fuzzymatch.loc[fuzzymatch[match_key].isnull(), mapping_matched_ind[match_key]] = 0\n",
    "    fuzzymatch.loc[fuzzymatch.mail_idx.isnull(), mapping_matched_ind[match_key]] = 0\n",
    "    del fuzzymatch['match_key']\n",
    "    return fuzzymatch\n",
    "\n",
    "# match_extract_with_campaign\n",
    "def f_match_extract_with_campaign(cp_abbr):\n",
    "    # %%time\n",
    "    # main #################################\n",
    "    maillist = extract_since_0123.copy()\n",
    "    maillist['mail_idx'] = np.array(range(maillist.shape[0]))\n",
    "    maillist['app_address_1'] = maillist.APP_ADDRESS_1.values\n",
    "\n",
    "    if cp_abbr=='jan23':\n",
    "        LS_SOL_0713_dedup = ls_sfl_jan23.copy() ################\n",
    "    elif cp_abbr=='feb5':\n",
    "        LS_SOL_0713_dedup = ls_sfl_feb5.copy() #################\n",
    "        \n",
    "    LS_SOL_0713_dedup = LS_SOL_0713_dedup.rename(columns={'key':'sfl_key'})\n",
    "    LS_SOL_0713_dedup['key'] = range(len(LS_SOL_0713_dedup))\n",
    "    LS0905_mail = LS_SOL_0713_dedup.copy()\n",
    "\n",
    "\n",
    "    maillist['zip'] = [str(int(x)) if str(x) not in ['nan','','(null)','NaT'] else 'tk_null' for x in maillist.APP_ZIP.values]\n",
    "    LS0905_mail['ZIP'] = [str(int(x)) if str(x) not in ['nan','','(null)','NaT'] else 'm_null' for x in LS0905_mail.ZIP.values]\n",
    "\n",
    "    match1 = f_match_key_generator(mail_data=maillist, tracking_data=LS0905_mail, match_key='firstname')\n",
    "    match2 = f_match_key_generator(mail_data=maillist, tracking_data=LS0905_mail, match_key='lastname') \n",
    "    match3 = f_match_key_generator(mail_data=maillist, tracking_data=LS0905_mail, match_key='addrs1')\n",
    "    match4 = f_match_key_generator(mail_data=maillist, tracking_data=LS0905_mail, match_key='geo_area')\n",
    "\n",
    "    match = match1.copy()\n",
    "    match = match.append(match2, ignore_index=True)\n",
    "    match = match.append(match3, ignore_index=True)\n",
    "    match = match.append(match4, ignore_index=True)\n",
    "\n",
    "    match['counter'] = 1\n",
    "    match = match.sort_values(['key','mail_idx'])\n",
    "    match = match.groupby(['key','mail_idx']).counter.count()\n",
    "    match = match.reset_index(drop=False)\n",
    "    match = match.merge(maillist[['mail_idx']].copy(), how='left', on='mail_idx')\n",
    "    match = match.sort_values(['key','counter'], ascending=[True, False])\n",
    "    match = match.groupby('key').first().reset_index(drop=False)\n",
    "\n",
    "    fuzzymatch = LS0905_mail[['key']].copy()\n",
    "    fuzzymatch = fuzzymatch.merge(match,how='left', on='key')\n",
    "    fuzzymatch = f_combine_matches(tracking_data=fuzzymatch, match=match, match_i=match1, match_key='firstname')\n",
    "    fuzzymatch = f_combine_matches(tracking_data=fuzzymatch, match=match, match_i=match2, match_key='lastname')\n",
    "    fuzzymatch = f_combine_matches(tracking_data=fuzzymatch, match=match, match_i=match3, match_key='addrs1')\n",
    "    fuzzymatch = f_combine_matches(tracking_data=fuzzymatch, match=match, match_i=match4, match_key='geo_area')\n",
    "\n",
    "    fuzzymatched = fuzzymatch.loc[(fuzzymatch.addrs1_matched_ind==1)\n",
    "                                 &((fuzzymatch.firstname_matched_ind==1)|(fuzzymatch.lastname_matched_ind==1))\n",
    "                                 &(fuzzymatch.geo_matched_ind==1)\n",
    "                                 &(~((fuzzymatch.firstname_matched_ind==0)&(fuzzymatch.lastname_matched_ind==0)))].copy()\n",
    "\n",
    "    #print('LS0905_mail.shape[0] is %i' %(LS0905_mail.shape[0]))\n",
    "    #print('fuzzymatch.shape[0] is %i' %(fuzzymatch.shape[0]))\n",
    "    #print('fuzzymatched.shape[0] is %i' %(fuzzymatched.shape[0]))\n",
    "    # print('fuzzymatch.loc[fuzzymatch.counter==4].shape[0] is %i' %(fuzzymatch.loc[fuzzymatch.counter==3].shape[0]))\n",
    "    \n",
    "    # combine data\n",
    "    maillist1 = maillist.copy()\n",
    "    maillist1 = f_remove_extract_fields(maillist1)\n",
    "\n",
    "    mailling = fuzzymatched.merge(maillist1, \n",
    "                                on='mail_idx', how='left') \n",
    "    responsed = LS0905_mail.merge(mailling, how='left', on='key')\n",
    "    responsed = responsed.loc[(~responsed.mail_idx.isnull())].copy()\n",
    "    responsed = responsed.reset_index(drop=True)\n",
    "\n",
    "    # match loan account\n",
    "    responsed = responsed.merge(keys.loc[(keys.days_diff_abs<30)&(keys.APP_PRODUCT_TYPE==keys.PRODUCT_TYPE),\n",
    "                            ['APP_APPLICATION_ID','ACCOUNT_ID']],\n",
    "                  how='left',\n",
    "                  on='APP_APPLICATION_ID')\n",
    "    responsed = responsed.merge(elms_all[['LOAN_AMOUNT','ACCOUNT_ID']],\n",
    "                               how='left',\n",
    "                               on='ACCOUNT_ID')\n",
    "\n",
    "\n",
    "    # match suchi perf\n",
    "    responsed = responsed.merge(sl_pf_v2, how='left', on='ACCOUNT_ID')\n",
    "\n",
    "    # funded_ind\n",
    "    responsed['funded_ind'] = 0\n",
    "    responsed.loc[(~responsed.ACCOUNT_ID.isnull()), 'funded_ind'] = 1\n",
    "\n",
    "    # key_type\n",
    "    responsed['key_type'] = \"Decision_Engine\"\n",
    "    responsed.loc[(responsed.funded_ind==1),'key_type'] = 'ELMS'\n",
    "\n",
    "    # response_flag\n",
    "    responsed['response_flag'] = 1\n",
    "    \n",
    "    return responsed\n",
    "\n",
    "# reformat\n",
    "def f_reformat(responsed_data):\n",
    "    extract_s2 = responsed_data.copy()\n",
    "    extract_part = pd.DataFrame({\n",
    "        'key': np.array(extract_s2['APP_APPLICATION_ID']),\n",
    "        'key_type': np.array(extract_s2['key_type']),\n",
    "        'First_Name': np.array(extract_s2['APP_FIRST_NAME']),\n",
    "        'Last_Name': np.array(extract_s2['APP_LAST_NAME']),\n",
    "        'Address1': np.array(extract_s2['APP_ADDRESS_1']),\n",
    "        'Address2': np.array(extract_s2['APP_ADDRESS_2']),\n",
    "        'Zip': np.array(extract_s2['APP_ZIP']),\n",
    "        'State': np.array(extract_s2['APP_STATE']),\n",
    "        'City': np.array(extract_s2['APP_CITY']),\n",
    "        'Phone': np.array(extract_s2['APP_CELL_PHONE_NUMBER']),\n",
    "        'Phone_Type': ['Cell' for x in range(len(extract_s2))],\n",
    "        'Email':np.array(extract_s2['APP_EMAIL_ADDRESS']),\n",
    "        'Channel': np.array(extract_s2['app_source_v2']),\n",
    "        'Response_Flag': np.array(extract_s2['response_flag']),\n",
    "        'Approved_Flag': np.array(extract_s2['approved_ind']),\n",
    "        'Product_Type': np.array(extract_s2['APP_PRODUCT_TYPE']),\n",
    "        'Application_Date': np.array(extract_s2['APPLICATION_DATE']),\n",
    "        'Approved_Loan_Amt1': np.array(extract_s2['DEC_LOAN_AMOUNT1']),\n",
    "        'APR1': np.array(extract_s2['DEC_LOAN_RATE1']),\n",
    "        'Term1': np.array(extract_s2['DEC_LOAN_TERM1_TXT']),\n",
    "        'Payment_Amount1': np.array(extract_s2['DEC_LOAN_PAYMENT_AMT1']),\n",
    "         'Approved_Loan_Amt2': np.array(extract_s2['DEC_LOAN_AMOUNT2']),\n",
    "        'APR2': np.array(extract_s2['DEC_LOAN_RATE2']),\n",
    "        'Term2': np.array(extract_s2['DEC_LOAN_TERM2_TXT']),\n",
    "        'Payment_Amount2': np.array(extract_s2['DEC_LOAN_PAYMENT_AMT2']),\n",
    "         'Approved_Loan_Amt3': np.array(extract_s2['DEC_LOAN_AMOUNT3']),\n",
    "        'APR3': np.array(extract_s2['DEC_LOAN_RATE3']),\n",
    "        'Term3': np.array(extract_s2['DEC_LOAN_TERM3_TXT']),\n",
    "        'Payment_Amount3': np.array(extract_s2['DEC_LOAN_PAYMENT_AMT3']),\n",
    "        'Payment_Frequency': np.array(extract_s2['DEC_LOAN_OFFER_TYPE']),\n",
    "        'Selected_Loan_Amt': np.array(extract_s2['SELECTED_LOAN_AMT']),\n",
    "        'Selected_Term': np.array(extract_s2['SELECTED_LOAN_TERMS']),\n",
    "        'Selected_APR': [np.nan for x in range(len(extract_s2))],\n",
    "        'Selected_Payment_Amount': [np.nan for x in range(len(extract_s2))],\n",
    "        'Actual_Loan_Amt': np.array(extract_s2['LOAN_AMOUNT']),\n",
    "        'Actual_Term': [np.nan for x in range(len(extract_s2))],\n",
    "        'Actual_APR': [np.nan for x in range(len(extract_s2))],\n",
    "        'Actual_Loan_Payment_Amount': np.array(extract_s2['REGULAR_PAYMENT_AMOUNT']),\n",
    "        'Pay_Amount': np.array(extract_s2['APP_PAY_AMOUNT']),\n",
    "        'Pay_Frequency': np.array(extract_s2['APP_PAY_FREQUENCY']),\n",
    "        'Govt_Assistance_Amt': np.array(extract_s2['APP_GOVT_ASSISTANCE_AMT']),\n",
    "        'Govt_Assistance_Frequency': np.array(extract_s2['APP_GOVT_ASSISTANCE_FREQUENCY']),\n",
    "        'Other_Income_Amt': np.array(extract_s2['APP_OTHER_INCOME_AMT']),\n",
    "        'Other_Income_Frequency': np.array(extract_s2['APP_OTHER_INCOME_FREQUENCY']),\n",
    "        'Calculated_Monthly_Income': np.array(extract_s2['monthly_income']),\n",
    "        'Age': np.array(extract_s2['age']),  \n",
    "        'SSN': np.array(extract_s2['APP_SSN']),\n",
    "        'PMT_DUE_IND': np.array(extract_s2['PMT_DUE_IND']),\n",
    "        'FPD': np.array(extract_s2['FPD']),\n",
    "        'DELIN_1': np.array(extract_s2['DELIN_1']),\n",
    "        'DELIN_7': np.array(extract_s2['DELIN_7']),\n",
    "        'DELIN_14': np.array(extract_s2['DELIN_14']),\n",
    "        'DELINBALANCE_1': np.array(extract_s2['DELINBALANCE_1']),\n",
    "        'DELINBALANCE_7': np.array(extract_s2['DELINBALANCE_7']),\n",
    "        'DELINBALANCE_14': np.array(extract_s2['DELINBALANCE_14']),\n",
    "        'Campaign': 'SFL ITA January',\n",
    "        'Loan_Account_ID': np.array(extract_s2['ACCOUNT_ID']),\n",
    "        'mkeyITA': np.array(extract_s2['mkeyITA']),\n",
    "        'Clarity_Score': np.array(extract_s2['CL_1_CF_score'])\n",
    "    })\n",
    "\n",
    "    # append\n",
    "    response_reformat = extract_part.copy()\n",
    "    response_reformat = response_reformat[fields].copy()\n",
    "    \n",
    "    return response_reformat\n",
    "\n",
    "# append weekly reformated data to history and update performance\n",
    "def f_append_history(responded_history_data, response_reformat_data):\n",
    "    # append to historical and update performance\n",
    "    perf_fields = ['PMT_DUE_IND', 'FPD','DELIN_1', 'DELIN_7', 'DELIN_14', 'DELINBALANCE_1','DELINBALANCE_7', 'DELINBALANCE_14']\n",
    "    responded_all = responded_history_data.append(response_reformat_data, ignore_index=True)\n",
    "\n",
    "    for each_perf_fields in perf_fields: \n",
    "        del responded_all[each_perf_fields]\n",
    "\n",
    "    responded_all = responded_all.merge(sl_pf_v2, left_on='Loan_Account_ID', right_on='ACCOUNT_ID', how='left')\n",
    "    \n",
    "    return responded_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# match extract with campaign database and add more fields\n",
    "responded_jan23 = f_match_extract_with_campaign(cp_abbr='jan23')\n",
    "responded_feb5 = f_match_extract_with_campaign(cp_abbr='feb5')\n",
    "\n",
    "# reformat responded_xx\n",
    "response_reformat_jan23 = f_reformat(responsed_data=responded_jan23)\n",
    "response_reformat_feb5 = f_reformat(responsed_data=responded_feb5)\n",
    "\n",
    "# append with history and update performance\n",
    "responded_all_jan23 = f_append_history(responded_history_data=responded_history_jan23, \n",
    "                                       response_reformat_data=response_reformat_jan23)\n",
    "responded_all_feb5 = f_append_history(responded_history_data=responded_history_feb5, \n",
    "                                       response_reformat_data=response_reformat_feb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# export\n",
    "responded_all_jan23[fields].to_excel(export_filename_jan23, index=False)\n",
    "responded_all_feb5[fields].to_excel(export_filename_feb5, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Append Clarity Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responded_all2 = responded_all.merge(extract_deduped[['APP_APPLICATION_ID','CL_1_CF_score']], \n",
    "                                    left_on='key', \n",
    "                                    right_on='APP_APPLICATION_ID',\n",
    "                                    how='left')\n",
    "responded_all2['Clarity_Score'] = [x if str(x)!='nan' else y \n",
    "                                   for (x,y) in zip(responded_all2.Clarity_Score, responded_all2.CL_1_CF_score)]\n",
    "responded_all2 = responded_all2[responded_all.columns.values].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SFL Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    data['app_source_v2'] = [np.nan if str(app_source) in ['nan','','(null)']\n",
    "                                    else (\"Lead Gen - \" + app_source if (str(app_source) in leadgen)\n",
    "                                                                     else ( \"Lead Gen - DDP Leads\" if (login_id.find(\"@DDPLEADS.COM\"))>-1\n",
    "                                                                                                   else(\"Call Center\" if (login_id.find(\"@LOANS2GOUSA.COM\"))>-1\n",
    "                                                                                                                      else (\"SFL\" if app_source=='SFL' \n",
    "                                                                                                                                  else \"Store\"))))\n",
    "                            for (app_source, login_id)\n",
    "                            in zip(data.APP_SOURCE, data.APP_LOGIN_ID_cap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_deduped_with_elms.APP_SOURCE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ".sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_deduped_with_elms.loc[(extract_deduped_with_elms.APP_SOURCE=='SFL')].groupby('app_branch_state').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 / (1 + np.exp(-(-0.859535 + (-0.222594) * ((0 - 0.534028) / 0.770519) + (-0.184440) * ((1 - 0.674306) / 0.468797) + 0.268756 * ((1 - 0.283333) / 0.450773) + 0.101492 * ((1 - 0.063889) / 0.244640) + (-0.239790) * ((1 - 0.584722) / 0.492941))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r extract_all_0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_all_0305.loc[(extract_all_0305.APP_APPLICATION_ID=='59fc8d64f1f179b5f401091e'),\n",
    "                    ['APP_APPLICATION_ID','DEC_FINAL_DECISION','approved_ind']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
